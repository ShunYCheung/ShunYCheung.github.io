---
title: 'Introduction to Gaussian processes'
date: 2025-11-22
permalink: /posts/2025/11/introduction_to_gaussian_processes/
tags:
  - Gaussian process
  - Code
---



I have used Gaussian process and it being used extensively in fields such as astrophysics. They are very useful and flexible models that can be applied to timeseries data. However, I often find it difficult to explain a Gaussian process (GP) to a non-expert in a short amount of time. While it is easy to use many of the GP libaries available, wrapping you head around the theoretical concepts can be challenging. 

In this blog post, I will give an introduction to Gaussian processes to a non-expert who are interested in learning and using this stastical tool.

# Weight-space Formulation

We start with a model within the Bayesian framework, then we can enhance this model by projecting the inputs into a higher dimensional feature space and apply our model there. We can apply a "kernel trick" to carry out the computation implicitly in the high dimensional space.

In our model $$\mu$$, we assume that there are a number of systematic effects that are not explicitly described by functions, but we know exists in the data. In other words, we don't know weight of each of those systematics. Hence, we assign an unknown weight $$w$$ for our systematics in our model

$$\begin{align}
f(\theta) = \mu(\theta) + A*w
\end{align}$$
where $$\theta$$ is the parameters, $$A$$ is called a design matrix wi and $$\epsilon$$ is white noise. We can define a standard likelihood for our data with Gaussian noise

$$\begin{align}
\mathcal{L}(d|\theta,\sigma, w) = \frac{1}{(2\pi C)}\exp(-\frac{1}{2}|y-f(\theta)|^T C^{-1}|y-f(\theta)|)
\end{align}$$

Next, we put a zero mean Gaussian prior with convariance matrix $$\Lambda$$ on the weights

$$\begin{align}
w \sim \mathcal{N}(0, \Lambda)
\end{align}$$

We can marginalise out $$w$$

$$\begin{align}
p(y|X) = \int p(y|X, w)p(w) dw
\end{align}$$

We write out the 

$$\begin{align}
p(y|X) = \int^\infty_{-\infty}\frac{1}{2\pi\Lambda}\exp(-\frac{w^T\Lambda w}{2})\frac{1}{(2\pi C)}\exp(-\frac{1}{2}|y-f(\theta)|^TC^{-1}|y-f(\theta)|)
\end{align}$$

Let $$h=\Sigma A^T C^{-1}(y-\mu(\theta))$$

$$\begin{align}
\int^\infty_{-\infty}\exp(-\frac{1}{2}|w-h|^T\Sigma^{-1}|w-h|)dw = |2\pi\Sigma|^{1/2}
\end{align}$$

$$\begin{align}
p(y|\theta) = \frac{1}{|2\pi(C+A\Lambda A^T)|^{1/2}}\exp(-\frac{1}{2}|y-\mu(\theta-\mu(\theta))|^T(C+A\Lambda A^T)^{-1}|y-\mu(\theta)|)
\end{align}$$

## Kernel trick

We define $$k(x, x')$$ as the kernel function, 
$$\begin{align}
k(x, x') = \phi(x)^T \Sigma_p \phi(x')
\end{align}$$


# Function-space Formulation

The formal definition of a Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. You can think of this as all the data is drawn from a huge multivariate Gaussian distribution. A Gaussian process is compeltely specified by the mean $$\mu(x)$$ and the covariance $$\Sigma(x, x')$$ and we can write the Gaussian process as

$$f(x)\sim GP(\mu(x), \Sigma(x, x'))$$



# Kernels


# Code packages and examples

There is a wide variety of Gaussian process python libaries with different implementations. 

Here is the list of the code packages
- [George](https://george.readthedocs.io/en/latest/): 
- [celerite](https://celerite.readthedocs.io/en/stable/): uses sums of complex-valued exponent as the kernel. This special kernel allows the computational cost of the GP to scale linearly with number of data points.
- [tinygp](https://tinygp.readthedocs.io/en/stable/): built on top of JAX, which allows for GPU acceleration and automatic differentiation.

# References
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/), Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X.