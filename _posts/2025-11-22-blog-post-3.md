---
title: 'Introduction to Gaussian processes'
date: 2025-11-22
permalink: /posts/2025/11/introduction_to_gaussian_processes/
tags:
  - Gaussian process
  - Code
---



I have used Gaussian process and it being used extensively in fields such as astrophysics. They are very useful and flexible models that can be applied to timeseries data. However, I often find it difficult to explain a Gaussian process (GP) to a non-expert in a short amount of time. While it is easy to use many of the GP libaries available, wrapping you head around the theoretical concepts can be challenging. 

In this blog post, I will give an introduction to Gaussian processes to a non-expert who are interested in learning and using this stastical tool.

# Weight-space Formulation

We start with a model within the Bayesian framework, then we can enhance this model by projecting the inputs into a higher dimensional feature space and apply our model there. We can apply a "kernel trick" to carry out the computation implicitly in the high dimensional space.

In our model $$\mu$$, we assume that there are a number of systematic effects that are not explicitly described by functions, but we know exists in the data. In other words, we don't know weight of each of those systematics. Hence, we assign an unknown weight $$w$$ for our systematics in our model

$$\begin{align}
f(\theta) = \mu(\theta) + A*w
\end{align}$$
where $$\theta$$ is the parameters, $$A$$ is called a design matrix. We can define a standard likelihood for our data with Gaussian noise

$$\begin{align}
\mathcal{L}(d|\theta,\sigma, w) = \frac{1}{(2\pi C)}\exp\left(-\frac{1}{2}|y-f(\theta)|^T C^{-1}|y-f(\theta)|\right)
\end{align}$$

Next, we put a zero mean Gaussian prior with convariance matrix $$\Lambda$$ on the weights

$$\begin{align}
w \sim \mathcal{N}(0, \Lambda)
\end{align}$$

We can marginalise out $$w$$

$$\begin{align}
p(y|X) = \int p(y|X, w)p(w) dw
\end{align}$$

which becomes

$$\begin{align}
p(y|X) = \int^\infty_{-\infty}\frac{1}{(2\pi\Lambda)(2\pi C)}\exp\left(-\frac{1}{2}(w^T\Lambda w+|y-f(\theta)|^TC^{-1}|y-f(\theta))|\right)
\end{align}$$

we take the argument in the exponent as
$$\begin{align}
z = w^T\Lambda w+|y-f(\theta)|^TC^{-1}|y-f(\theta)|
\end{align}$$
and complete the square so that the argument becomes

$$\begin{align}
z = -(w-h)^T\Sigma (w-h) + k
\end{align}$$

where 

$$\begin{align}
\Sigma^{-1} = \Lambda^{-1} + A^{T}C^{-1}A\\
h=\Sigma A^T C^{-1}(y-\mu(\theta))\\
k=(y-\mu(\theta))^T(C^{-1}-C^{-1}A\Sigma A^T C^{-1})(y-\mu(\theta))
\end{align}$$

and thus we can rewrite our integral

$$\begin{align}
p(y|\theta) = \frac{1}{(2\pi\Lambda)(2\pi C)}\exp\left(-\frac{k}{2}\right)\int^\infty_{-\infty}\exp\left(-\frac{1}{2}[w-h]^T\Sigma^{-1}[w-h]\right).
\end{align}$$


The solution to a integral of a multivariate Gaussian is

$$\begin{align}
\int^\infty_{-\infty}\exp\left(-\frac{1}{2}|w-h|^T\Sigma^{-1}|w-h|\right)dw = |2\pi\Sigma|^{1/2}
\end{align}$$

and using 

$$\begin{align}
|\Sigma| = \frac{|\Lambda C|}{|C + A\Lambda A^T|}
\end{align}$$

which finally gives our expression

$$\begin{equation}
p(y|\theta) = \frac{1}{|2\pi(C+A\Lambda A^T)|^{1/2}}\exp\left(-\frac{1}{2}|y-\mu(\theta)|^T(C+A\Lambda A^T)^{-1}|y-\mu(\theta)|\right)
\end{equation}$$

## Kernel trick

The problem with our model is that it is not flexible enough. Our model is linear but many problems in the real world are non-linear in nature. Hence, the model cannot map and fully describe the data. To overcome this problem, we can project our data from the lower dimensional data space into a higher dimensional feature space where there may be linear relations between the data and allow us to make non-linear predictions in the data space.

But now, we have a problem where we need a infinitely large matrix to represent this higher dimensional space. However, due [Mercer's theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem), where any inner product of matrices can be replaced by a function as long as it returns a positive semi-definite matrix. This function is the kernel function,

$$\begin{align}
k(x, x') = A^T \Lambda A
\end{align}$$

This is very convenient as we can recast our problem in terms of the kernel function instead of computing the inner product of infinite dimensional matrices. 

# Function-space Formulation

The formal definition of a Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. You can think of this as all the data is drawn from a huge multivariate Gaussian distribution. A Gaussian process is compeltely specified by the mean $$\mu(x)$$ and the covariance $$\Sigma(x, x')$$ and we can write the Gaussian process as

$$f(x)\sim GP(\mu(x), \Sigma(x, x'))$$



# Kernels

We have shown that the data in the higher dimensional space can be described by a kernel function, but what kernel functions can we use. Kernels can be classified into stationary or non-stationary. Stationary kernels depends the relative distance between data points, which you can think of modelling an effect independent of time. On the other hands, non-stationary kernels depend on the value of the input value, which models an effect at a specific, local point in time.  

There are few different types of kernels which can be used as is or in combination with other kernels:

Each kernel function has its own parameters (we call them hyperparameters). They do not directly describe the data, but describe the structure within the covariance matrix, which then describes the data.  



# Code packages and examples

There are a wide variety of Gaussian process python libaries with different implementations. The greatest challenge to computing a GP comes from inverting the covariance matrix $$C+A\Lambda A^T$$, which has the computational complexity of $$\mathcal{O}(n^3)$$. Hence, the aim of the game is to reduce the complexity down. Some implementations is able to do this by using certain mathematical property's under special conditions.

Here is the list of the code packages
- [George](https://george.readthedocs.io/en/latest/): python libary that implements GPs. You can use both stationary and non-stationary kernels. It primarily use two solvers, which are [scipy's Cholesky implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cholesky.html) and [Sivaram Amambikasaran's HOLDLR algorithm](https://arxiv.org/abs/1403.6015).
- [celerite](https://celerite.readthedocs.io/en/stable/): uses sums of complex-valued exponent as the kernel. This special kernel allows the computational cost of the GP to scale linearly with number of data points.
- [tinygp](https://tinygp.readthedocs.io/en/stable/): built on top of JAX, which allows for GPU acceleration and automatic differentiation.

 


# References
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/), Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X.