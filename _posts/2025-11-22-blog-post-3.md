---
title: 'Introduction to Gaussian processes'
date: 2025-11-22
permalink: /posts/2025/11/introduction_to_gaussian_processes/
tags:
  - Gaussian process
  - Code
---



I have used Gaussian process and it being used extensively in fields such as astrophysics. They are very useful and flexible models that can be applied to timeseries data. However, I often find it difficult to explain a Gaussian process (GP) to a non-expert in a short amount of time. While it is easy to use many of the GP libaries available, wrapping you head around the theoretical concepts can be challenging. 

In this blog post, I will give an introduction to Gaussian processes to a non-expert who are interested in learning and using this stastical tool.

# Weight-space Formulation

We start with a model within the Bayesian framework, then we can enhance this model by projecting the inputs into a higher dimensional feature space and apply our model there. We can apply a "kernel trick" to carry out the computation implicitly in the high dimensional space.

In our model $$\mu$$, we assume that there are a number of systematic effects that are not explicitly described by functions, but we know exists in the data. In other words, we don't know weight of each of those systematics. Hence, we assign an unknown weight $$w$$ for our systematics in our model

$$\begin{align}
f(\theta) = \mu(\theta) + A*w
\end{align}$$
where $$\theta$$ is the parameters, $$A$$ is called a design matrix. We can define a standard likelihood for our data with Gaussian noise

$$\begin{align}
\mathcal{L}(d|\theta,\sigma, w) = \frac{1}{(2\pi C)}\exp\left(-\frac{1}{2}|y-f(\theta)|^T C^{-1}|y-f(\theta)|\right)
\end{align}$$

Next, we put a zero mean Gaussian prior with convariance matrix $$\Lambda$$ on the weights

$$\begin{align}
w \sim \mathcal{N}(0, \Lambda)
\end{align}$$

We can marginalise out $$w$$

$$\begin{align}
p(y|X) = \int p(y|X, w)p(w) dw
\end{align}$$

which becomes

$$\begin{align}
p(y|X) = \int^\infty_{-\infty}\frac{1}{(2\pi\Lambda)(2\pi C)}\exp\left(-\frac{1}{2}(w^T\Lambda w+|y-f(\theta)|^TC^{-1}|y-f(\theta))|\right)
\end{align}$$

we take the argument in the exponent as
$$\begin{align}
z = w^T\Lambda w+|y-f(\theta)|^TC^{-1}|y-f(\theta)|
\end{align}$$
and complete the square so that the argument becomes

$$\begin{align}
z = -(w-h)^T\Sigma (w-h) + k
\end{align}$$

where 

$$\begin{align}
\Sigma^{-1} = \Lambda^{-1} + A^{T}C^{-1}A\\
h=\Sigma A^T C^{-1}(y-\mu(\theta))\\
k=(y-\mu(\theta))^T(C^{-1}-C^{-1}A\Sigma A^T C^{-1})(y-\mu(\theta))
\end{align}$$

and thus we can rewrite our integral

$$\begin{align}
p(y|\theta) = \frac{1}{(2\pi\Lambda)(2\pi C)}\exp\let(-\frac{k}{2}\right)\int^\infty_{-\infty}\exp\left(-\frac{1}{2}[w-h]^T\Sigma^{-1}[w-h]\right).
\end{align}$$


The solution to a integral of a multivariate Gaussian is

$$\begin{align}
\int^\infty_{-\infty}\exp\left(-\frac{1}{2}|w-h|^T\Sigma^{-1}|w-h|\right)dw = |2\pi\Sigma|^{1/2}
\end{align}$$

and using 

$$\begin{align}
|\Sigma| = \frac{|\Lambda C|}{|C + A\Lambda A^T|}
\end{elign}$$

which finally gives our expression

$$\begin{align}
p(y|\theta) = \frac{1}{|2\pi(C+A\Lambda A^T)|^{1/2}}\exp\left(-\frac{1}{2}|y-\mu(\theta)|^T(C+A\Lambda A^T)^{-1}|y-\mu(\theta)|\right)
\end{align}$$

## Kernel trick

The problem with our model is that it is not flexible enough. Our model is linear but many problems in the real world are non-linear in nature. Hence, the model cannot map and fully describe the data. To overcome this problem, we can project our data from the lower dimensional data space into a higher dimensional feature space where there may be linear relations between the data and allow us to make non-linear predictions in the data space.

But now, we have a problem where we need a infinitely large matrix to represent this higher dimensional space. However, due to a unique property of Hilbert spaces, where any inner product of matrices can be replacesd by a function as long as it returns a positive semi-definite matrix. This function is the kernel function,

$$\begin{align}
k(x, x') = A^T \Lambda A
\end{align}$$

This is very convenient as we can recast our problem in terms of the kernel function instead of computing the inner product of infinite dimensional matrices. 

# Function-space Formulation

The formal definition of a Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. You can think of this as all the data is drawn from a huge multivariate Gaussian distribution. A Gaussian process is compeltely specified by the mean $$\mu(x)$$ and the covariance $$\Sigma(x, x')$$ and we can write the Gaussian process as

$$f(x)\sim GP(\mu(x), \Sigma(x, x'))$$



# Kernels


# Code packages and examples

There is a wide variety of Gaussian process python libaries with different implementations. 

Here is the list of the code packages
- [George](https://george.readthedocs.io/en/latest/): 
- [celerite](https://celerite.readthedocs.io/en/stable/): uses sums of complex-valued exponent as the kernel. This special kernel allows the computational cost of the GP to scale linearly with number of data points.
- [tinygp](https://tinygp.readthedocs.io/en/stable/): built on top of JAX, which allows for GPU acceleration and automatic differentiation.

# References
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/), Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X.