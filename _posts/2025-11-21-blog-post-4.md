---
title: 'Is normalising flows the future?'
date: 2025-11-22
permalink: /posts/2025/11/normalising_flows/
tags:
  - Normalising flows
  - Code
---


In the field of Gravitational-Wave astronomy, I see a transition from traditional stochastic sampling methods such as Monte Carlo Markov Chains and nested sampling towards something called *normalising flows*. As our inference problems become harder due to the increasing volume of data, we need faster inference methods. In this blog post, we will go through what is a normalising flow and can it solve inference problem that are intractably long for traditional methods. 

# Theoretical background

A normalising flows works by transforming a simple probabilty distribution (usually a Gaussian distribution) into a more complex distribution through a sequence of mappings. Let $$X$$ be a random variable with a tractable probability density function $$p_X$$. We want to transform this distribution $$p_X$$ (source distribution) into distribution $$p_Y$$ (target distribution) using an invertible (bijective) function $$g$$ where

$$\begin{align}
Y = g(X).
\end{align}$$

We can also transform $$p_Y$$ back into $$p_X$$ using the inverse function $$g^{-1}$$. 


## Change of variables

If we want to figure out the probability value close to a particular Y value, we look at the area under a small interval $$dx$$. We can do something similar for X. 

$$\begin{align}
p_Y(y)|\partial y| = p_X(x)|\partial x|
\end{align}$$

$$\begin{align}
p_Y(y) = p_X(x)|\frac{\partial x}{\partial y}|
\end{align}$$

$$\begin{align}
p_Y(y) = p_X(x)|\frac{\partial y}{\partial x}|^{-1}
\end{align}$$

substituting $$x$$ with the inverse function $$g^{-1}(y)=x$$, we have the formula

$$\begin{equation}
p_Y(y) = p_X(g^{-1}(y))\left|\frac{\partial g(g^{-1}(y))}{\partial x}\right|.
\end{equation}$$

where the term $$\vert \frac{\partial g(g^{-1}(y))}{\partial x}\vert$$ is the Jacobian. One job for the Jacobian is to ensure the new probability density *normalises* to 1. The new probability density function $$p_Y(y)$$ is called a *pushforward* of the density $$p_X$$.

The function $$g$$ (the generator) pushes forward the original density $$p_X$$ to a more complex density. This movement where the simple density *flows* to a more complex density it called the *generative direction*. We can take the inverse function $$g^{-1}$$ and use it to move in the opposite direction, turning from a complex distribution into a more simple distribution. Hence, this is why it is named *normalising flows*, we applying invertible transformations to transform probability distributions while ensuring they are normalised.


## Generative model likelihood

An obvious use for normalising flows is to perform parameter estimation. We assume that there is a only a single flow parameterised by $$\theta$$ and the original distribution is parameterised by $$\phi$$. We need to derive a likelihood function to evaluate the data with given parameters $$\psi = (\theta. \phi)$$. We can simply take the previous equation and take the logarithm on both sides, which results in our log likelihood function

$$\begin{align}
\log p_Y(y|\psi) = \log p_X(g^{-1}(y|\theta)|\phi) + \log\left|\det \frac{\partial g(g^{-1}(y|\theta))}{\partial x} \right|
\end{align}$$

where the first term is the log likelihood under the original distribution and the second term accounts for the change of volume induced by the transformation of the normalising flows. 

# Implementing normalising flows

Normalising flows should satisfy the following conditions in order to be practical:
- be invertible.
- be sufficiently expressive to model the distribution of interest.
- be computationally efficient, at computing $$g$$, $$g^{-1}$$ and also the determinant of the Jacobian. 

Constructing a complicated bijective function can be difficult. One approach is to use a simpler bijective function to compose a much more complicated function. A composite function $$g$$ made of $$N$$ invertible function $$\{g_i\}_{i=0}^{N}$$ is itself invertible and the determinant of its Jacobian has a specific form. Hence, $$g$$ is bijective and the inverse

$$\begin{align}
g^{-1} = g^{-1}_0 \circ g^{-1}_1 \circ ... \circ g^{-1}_N
\end{align}$$

and the determinant of the Jacobian is 

$$\begin{align}
\det\left( \frac{\partial g(g^{-1}(y|\theta))}{\partial x}\right) = \prod^N_{i=1} \det\left(\frac{\partial g(g_i^{-1}(y|\theta))}{\partial x}\right).
\end{align}$$

Thus, we can use multiple bijective functions to compose a more complicated function. 

## Coupling flows

A trick for reducing the computational cost of a flow is to ensure that the Jacobian is triangular (where the $$i>j$$ or $$i<j$$ elements in a matrix is zero). Since we wish to calculate the determinant of the Jacobian in our likelihood function, only the diagonal of the matrix will give a non-zero value as the upper (or lower) triangular of the matrix has zero values, hence the off-diagonal terms in the determinant calculation is zero. 

One way to ensure the Jacobian is triangular is by using a coupling layer. Consider the input $$x \in \mathcal{R}$$ into two subspaces $$x_A$$ and $$x_B$$ and a bijective function $$h$$. For the first subset, we use two functions, $$h$$ and $$\Theta$$, where $$\Theta$$ is an input to $$h$$:

$$\begin{align}
y_A = h(x_A: \Theta(x_B))
\end{align}$$

$$\begin{align}
y_B = x_B
\end{align}$$

and we also need $$h$$ to be invertible so that 

$$\begin{align}
x_A = h^{-1}(y_A: \Theta(x_B))
\end{align}$$

$$\begin{align}
x_B = y_B
\end{align}$$

The graph below visually demonstrates how the coupling layer works ![(Credit: Ivan *et al.* (2019))]({{ '/assets/images/coupling_flows.png' | relative_url}}). 

In this particular transformation, the Jacobian will be upper tringular:

$$\begin{align}
\begin{pmatrix}
\frac{\partial y_A}{\partial x_A} & \frac{\partial y_A}{\partial x_B}\\
0 & I
\end{pmatrix}
\end{align}$$

as $$y_B = x_B$$ implies that lower right element is 1 and as the second subset is completely independent of the first subset, the lower left element is 0. 


### Affine coupling

Now that we introduce coupling flows, a question that natually arises is what should the coupling function $$h$$ be? One architectures called
nonlinear independent component (NICE) proposed by Dinh *et al*. (2015) has a simple additive function:

$$\begin{align}
h(x_A, \Theta(x_B)) = x_A + \Theta(x_B)
\end{align}$$
and real non-volume preserving (R-NVP) flows
$$\begin{align}
h(x_A, \Theta(x_B)) = x_A + T(x_B)
\end{align}$$

where $$T$$ is known as a translation function. Another architecture called real non-volume preserving (R-NVP) flow proposed by Dinh *et al*. (2017) uses an affine coupling function:

$$\begin{align}
h(x_A, \Theta(x_B)) = \Theta_1(x_B)x + \Theta_2(x_B)
\end{align}$$

$$\begin{align}
h(x_A, \Theta(x_B)) = \exp(S(x_B))\circ x + T(x_B)
\end{align}$$

where $$T$$ is the same translation function as before and $$S$$ is a scaling function. The functions are both simple and easier to compute. However, the drawback is that you need to stack many functions together to respresnt complicated distributions. 

# Code example

**In progress**

# References
- [Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257), Ivan *et al.* (2019)
- [Normalizing flows: Explicit distribution modeling](https://medium.com/@mechatronics420/normalizing-flows-introduction-ad1888771890), Ashish Jha, 12/10/2023 
- [What are normalising flows](https://www.youtube.com/watch?v=i7LjDvsLWCg&t=116s), Ari Seff, 7/12/2019
- [Normalizing Flows](https://grishmaprs.medium.com/normalizing-flows-5b5a713e45e2), Grishma Prasad (2021).
- [NICE: Non-linear Independent Components Estimation](https://arxiv.org/abs/1410.8516), Dinh *et al*. (2015)
- [Density estimation using Real NVP](https://arxiv.org/abs/1605.08803), Dinh *et al*. (2017)
