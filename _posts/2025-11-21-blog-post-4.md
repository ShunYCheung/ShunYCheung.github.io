---
title: 'Is normalising flows the future?'
date: 2025-11-22
permalink: /posts/2025/11/normalising_flows/
tags:
  - Normalising flows
  - Code
---


In the field of Gravitational-Wave astronomy, I see a transition from traditional stochastic sampling methods such as Monte Carlo Markov Chains and nested sampling towards something called *normalising flows*. As our inference problems become harder due to the increasing volume of data, we need faster inference methods. In this blog post, we will go through what is a normalising flow and can it solve inference problem that are intractably long for traditional methods. 

A normalising flows works by transforming a simple probabilty distribution (usually a Gaussian distribution) into a more complex distribution through a sequence of mappings. Let $$X$$ be a random variable with a tractable probability density function $$p_X$$. We want to transform this distribution $$p_X$$ (source distribution) into distribution $$p_Y$$ (target distribution) using an invertible (bijective) function $$g$$ where

$$\begin{align}
Y = g(X).
\end{align}$$

We can also transform $$p_Y$$ back into $$p_X$$ using the inverse function $$g^{-1}$$. 


## Change of variables

If we want to figure out the probability value close to a particular Y value, we look at the area under a small interval $$dx$$. We can do something similar for X. 

$$\begin{align}
p_Y(y)|\partial y| = p_X(x)|\partial x|
\end{align}$$

$$\begin{align}
p_Y(y) = p_X(x)|\frac{\partial x}{\partial y}|
\end{align}$$

$$\begin{align}
p_Y(y) = p_X(x)|\frac{\partial y}{\partial x}|^{-1}
\end{align}$$

substituting $$x$$ with the inverse function $$g^{-1}(y)=x$$, we have the formula

$$\begin{equation}
p_Y(y) = p_X(g^{-1}(y))\left|\frac{\partial g(g^{-1}(y))}{\partial x}\right|.
\end{equation}$$

where the term $$\vert \frac{\partial g(g^{-1}(x))}{\partial x}\vert$$ is the Jacobian. One job for the Jacobian is to ensure the new probability density *normalises* to 1. The new probability density function $$p_Y(y)$$ is called a *pushforward* of the density $$p_X$$.

The function $$g$$ (the generator) pushes forward the original density $$p_X$$ to a more complex density. This movement where the simple density *flows* to a more complex density it called the *generative direction*. We can take the inverse function $$g^{-1}$$ and use it to move in the opposite direction, turning from a complex distribution into a more simple distribution. Hence, this is why it is named *normalising flows*, we applying invertible transformations to transform probability distributions while ensuring they are normalised.

## Generative model likelihood

$$\begin{align}
\log p_Y(y|\theta) = \log p_X(g^{-1}(y|\theta)|\phi) + \log\left|\det \frac{\partial g(g^{-1}(y))}{\partial x} \right|
\end{align}$$


**In progress**

# References
- [Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257), Ivan *et al.* (2019)
- [Normalizing flows: Explicit distribution modeling](https://medium.com/@mechatronics420/normalizing-flows-introduction-ad1888771890), Ashish Jha, 12/10/2023 
- [What are normalising flows](https://www.youtube.com/watch?v=i7LjDvsLWCg&t=116s), Ari Seff, 7/12/2019

