---
title: 'Exploring factors and causes in student stress using a clustering algorithm'
date: 2025-10-22
permalink: /posts/2025/10/blog-post-1/
tags:
  - Data Analysis
  - Clustering algorithm
  - Code
---

Recently, I came across this kaggle dataset for student stress suvery conducted nationwide. There are 20 factors where participants must give a numerical score to rate how prevalent these factors are. This dataset is pretty interesting as stress is something that affects us all, and understanding what factors are correlated with increased stress levels can improve our quality of life.

# Introduction

For this analysis, I will be using a clustering algorithm. Clustering groups the data points together based on a metric. I will be using a clustering algorithm called Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). It is a slightly modified version of DBSCAN, density-based clustering algorithm, which groups the data points in clusters of high density. However, the HDBSCAN further sorts these clusters into a hierarchy tree, and extracts only the clusters that are most stable.


# Code

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import hdbscan

args = dict(csv = )
csv_path = Path(args['csv'])
outdir = Path(args.outdir) if args.outdir else csv_path.parent
outdir.mkdir(parents=True, exist_ok=True)

# Load data
df = pd.read_csv(csv_path)

# Choose feature columns
if args.cols is None:
    cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(cols) == 0:
        raise ValueError("No numeric columns found. Specify --cols explicitly.")
else:
    for c in args.cols:
        if c not in df.columns:
            raise ValueError(f"Column '{c}' not found in CSV.")
    cols = args.cols

X = df[cols].copy()

# Optionally drop rows with NA in selected columns
if args.drop_na_rows:
    mask = X.notna().all(axis=1)
    df = df.loc[mask].reset_index(drop=True)
    X = X.loc[mask].reset_index(drop=True)

# Impute + scale
X_imp = SimpleImputer(strategy="median").fit_transform(X)
X_scaled = StandardScaler().fit_transform(X_imp)

# HDBSCAN
min_samples = args.min_samples if args.min_samples is not None else args.min_cluster_size
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=args.min_cluster_size,
    min_samples=min_samples,
    metric=args.metric,
    cluster_selection_epsilon=args.cluster_selection_epsilon,
    cluster_selection_method="eom",
    allow_single_cluster=args.allow_single_cluster,
    core_dist_n_jobs=1  # robust default across platforms
)
labels = clusterer.fit_predict(X_scaled)

# Save labeled CSV with probabilities and outlier scores
out_csv = outdir / f"{csv_path.stem}_hdbscan_clusters.csv"
```

# Results

After using 

First, we can visualise how the clusters are separated. One way is to perform principal component analysis, and plot the clusters with x and y axes being the two principal components.

![Visualisation of clusters using principal component analysis](..\images\StressLevelDataset_hdbscan_pca.png)

